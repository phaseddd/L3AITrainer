# å®æ“é¢˜2è¶…è¯¦ç»†æ•™å­¦æŒ‡å— - Pythonå°ç™½ç‰ˆ

## ğŸ¯ é¢˜ç›®èƒŒæ™¯
ä½ è¦åšä¸€ä¸ª**é‚®ä»¶åƒåœ¾åˆ†ç±»å™¨**ï¼Œå°±åƒQQé‚®ç®±è‡ªåŠ¨æŠŠåƒåœ¾é‚®ä»¶æ”¾åˆ°åƒåœ¾ç®±ä¸€æ ·ã€‚

**æ•°æ®æ ·ä¾‹**ï¼š
```
text,label
"Win a brand new iPhone 12! Click here to claim your prize now!",1  â† åƒåœ¾é‚®ä»¶
"Hi John, let's catch up for lunch tomorrow at 1 PM.",0              â† æ­£å¸¸é‚®ä»¶
```
- `text`: é‚®ä»¶å†…å®¹
- `label`: 0=æ­£å¸¸é‚®ä»¶ï¼Œ1=åƒåœ¾é‚®ä»¶

## ğŸ“š PythonåŸºç¡€è¯­æ³•é€Ÿæˆï¼ˆJavaå¯¹æ¯”ï¼‰

### 1. å˜é‡å’Œæ•°æ®ç±»å‹
```python
# Python - ä¸éœ€è¦å£°æ˜ç±»å‹
name = "å¼ ä¸‰"           # å­—ç¬¦ä¸²
age = 25               # æ•´æ•°
scores = [90, 85, 88]  # åˆ—è¡¨ï¼ˆç›¸å½“äºJavaçš„ArrayListï¼‰
info = {"name": "å¼ ä¸‰", "age": 25}  # å­—å…¸ï¼ˆç›¸å½“äºJavaçš„HashMapï¼‰

# Javaå¯¹æ¯”
String name = "å¼ ä¸‰";
int age = 25;
List<Integer> scores = Arrays.asList(90, 85, 88);
Map<String, Object> info = new HashMap<>();
```

### 2. å¯¼å…¥åº“ï¼ˆç›¸å½“äºJavaçš„importï¼‰
```python
import pandas as pd        # å¯¼å…¥pandasåº“ï¼Œèµ·åˆ«åpd
from sklearn.model_selection import train_test_split  # ä»sklearnåº“å¯¼å…¥ç‰¹å®šå‡½æ•°

# Javaå¯¹æ¯”
import java.util.*;
import java.util.List;
```

### 3. å‡½æ•°å®šä¹‰
```python
# Python
def clean_text(text):      # def = å®šä¹‰å‡½æ•°
    return text.lower()    # è¿”å›å°å†™æ–‡æœ¬

# Javaå¯¹æ¯”
public String cleanText(String text) {
    return text.toLowerCase();
}
```

### 4. å¾ªç¯å’Œæ¡ä»¶
```python
# Python - ç”¨ç¼©è¿›è¡¨ç¤ºä»£ç å—
for item in my_list:       # éå†åˆ—è¡¨
    if item > 10:          # å¦‚æœæ¡ä»¶
        print(item)        # æ‰“å°

# Javaå¯¹æ¯”
for (String item : myList) {
    if (item > 10) {
        System.out.println(item);
    }
}
```

## ğŸ”§ å…³é”®Pythonåº“è¯¦è§£

### 1. pandas - æ•°æ®å¤„ç†ç¥å™¨
```python
import pandas as pd

# è¯»å–CSVæ–‡ä»¶ï¼ˆç›¸å½“äºJavaè¯»å–æ•°æ®åº“è¡¨ï¼‰
data = pd.read_csv("æ–‡ä»¶è·¯å¾„.csv")

# æŸ¥çœ‹æ•°æ®
print(data.head())      # æ˜¾ç¤ºå‰5è¡Œ
print(data.shape)       # æ˜¾ç¤ºè¡Œæ•°å’Œåˆ—æ•° (è¡Œ, åˆ—)
print(data['text'])     # è·å–textåˆ—çš„æ‰€æœ‰æ•°æ®

# åº”ç”¨å‡½æ•°åˆ°æ¯ä¸€è¡Œ
data['text'] = data['text'].apply(clean_text)  # å¯¹textåˆ—çš„æ¯ä¸ªå€¼åº”ç”¨clean_textå‡½æ•°

# ä¿å­˜æ–‡ä»¶
data.to_csv("è¾“å‡ºæ–‡ä»¶.csv", index=False)  # index=Falseè¡¨ç¤ºä¸ä¿å­˜è¡Œå·
```

### 2. re - æ­£åˆ™è¡¨è¾¾å¼
```python
import re

text = "Hello, World! 123"
# å»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—çš„å­—ç¬¦
clean = re.sub(r'[^\w\s]', '', text)  # ç»“æœ: "Hello World 123"
# r'[^\w\s]' è§£é‡Šï¼š
# r'' = åŸå§‹å­—ç¬¦ä¸²
# [^...] = ä¸åŒ…å«...çš„å­—ç¬¦
# \w = å­—æ¯æ•°å­—ä¸‹åˆ’çº¿
# \s = ç©ºæ ¼
# æ‰€ä»¥è¿™ä¸ªè¡¨è¾¾å¼æ„æ€æ˜¯ï¼šåˆ é™¤æ‰€æœ‰ä¸æ˜¯å­—æ¯æ•°å­—ç©ºæ ¼çš„å­—ç¬¦
```

### 3. nltk - è‡ªç„¶è¯­è¨€å¤„ç†
```python
import nltk
from nltk.corpus import stopwords

# ä¸‹è½½åœç”¨è¯è¡¨
nltk.download('stopwords')

# è·å–è‹±æ–‡åœç”¨è¯
stop_words = stopwords.words('english')  # ['the', 'a', 'an', 'and', ...]

# å»é™¤åœç”¨è¯
words = "the quick brown fox".split()  # ['the', 'quick', 'brown', 'fox']
filtered = [word for word in words if word not in stop_words]  # ['quick', 'brown', 'fox']
```

## ğŸ“ ä¸‰ä¸ªæ­¥éª¤è¶…è¯¦ç»†è§£æ

### æ­¥éª¤1: æ•°æ®é¢„å¤„ç† (2-1åˆ’åˆ†æ•°æ®.py)

#### ğŸ” é€è¡Œä»£ç è§£é‡Š

```python
# ç¬¬1-8è¡Œï¼šå¯¼å…¥éœ€è¦çš„åº“
import pandas as pd           # æ•°æ®å¤„ç†åº“
import re                     # æ­£åˆ™è¡¨è¾¾å¼åº“
import os                     # æ“ä½œç³»ç»Ÿæ¥å£åº“
import nltk                   # è‡ªç„¶è¯­è¨€å¤„ç†åº“
nltk.download('stopwords')    # ä¸‹è½½åœç”¨è¯æ•°æ®

from nltk.corpus import stopwords                    # å¯¼å…¥åœç”¨è¯
from sklearn.model_selection import train_test_split # å¯¼å…¥æ•°æ®åˆ†å‰²å‡½æ•°
```

```python
# ç¬¬10è¡Œï¼šè¯»å–æ•°æ®
data = pd.read_csv("")  # ğŸš¨ è€ƒè¯•æ—¶éœ€è¦å¡«å…¥ï¼šæ¡Œé¢/èµ„æºåŒ…/é¢˜ç›®2/é‚®ä»¶æ•°æ®.csv

# ç›¸å½“äºJavaçš„ï¼š
// ResultSet data = statement.executeQuery("SELECT * FROM emails");
```

```python
# ç¬¬12-17è¡Œï¼šå®šä¹‰æ–‡æœ¬æ¸…æ´—å‡½æ•°
def clean_text(text):
    # å»é™¤ç‰¹æ®Šç¬¦å·ï¼šåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼
    text = re.sub(r'[^\w\s]', '', text)  
    
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()  
    
    # å»é™¤åœç”¨è¯
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    
    return text

# è¯¦ç»†è§£é‡Šæœ€åä¸€è¡Œï¼š
# text.split()                    # æŠŠæ–‡æœ¬æŒ‰ç©ºæ ¼åˆ†å‰²æˆå•è¯åˆ—è¡¨
# word for word in text.split()   # éå†æ¯ä¸ªå•è¯
# if word not in stopwords.words('english')  # å¦‚æœå•è¯ä¸åœ¨åœç”¨è¯è¡¨ä¸­
# [...]                          # åˆ—è¡¨æ¨å¯¼å¼ï¼Œç”Ÿæˆæ–°åˆ—è¡¨
# ' '.join([...])                # æŠŠåˆ—è¡¨é‡æ–°ç”¨ç©ºæ ¼è¿æ¥æˆå­—ç¬¦ä¸²
```

```python
# ç¬¬19è¡Œï¼šåº”ç”¨æ¸…æ´—å‡½æ•°
data['text'] = data['text'].apply(clean_text)

# ç›¸å½“äºJavaçš„ï¼š
// for (int i = 0; i < data.size(); i++) {
//     data.get(i).setText(cleanText(data.get(i).getText()));
// }
```

```python
# ç¬¬21è¡Œï¼šæ•°æ®åˆ’åˆ†
train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)

# å‚æ•°è§£é‡Šï¼š
# data: è¦åˆ†å‰²çš„æ•°æ®
# test_size=0.3: æµ‹è¯•é›†å 30%ï¼Œè®­ç»ƒé›†å 70%
# random_state=42: éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡åˆ†å‰²ç»“æœä¸€æ ·
```

```python
# ç¬¬23-26è¡Œï¼šä¿å­˜ç»“æœ
os.makedirs("", exist_ok=True)  # ğŸš¨ å¡«å…¥ï¼šC:/Project/2/
train_data.to_csv("", index=False)  # ğŸš¨ å¡«å…¥ï¼šC:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_train.csv
test_data.to_csv("", index=False)   # ğŸš¨ å¡«å…¥ï¼šC:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_test.csv

# os.makedirsè§£é‡Šï¼š
# åˆ›å»ºç›®å½•ï¼Œexist_ok=Trueè¡¨ç¤ºå¦‚æœç›®å½•å·²å­˜åœ¨ä¸æŠ¥é”™
```

#### âš ï¸ è€ƒè¯•å¡«ç©ºç­”æ¡ˆï¼š
```python
# ç¬¬10è¡Œ
data = pd.read_csv("C:/Users/ç”¨æˆ·å/Desktop/èµ„æºåŒ…/é¢˜ç›®2/é‚®ä»¶æ•°æ®.csv")

# ç¬¬24è¡Œ  
os.makedirs("C:/Project/2/", exist_ok=True)

# ç¬¬25è¡Œ
train_data.to_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_train.csv", index=False)

# ç¬¬26è¡Œ
test_data.to_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_test.csv", index=False)
```

### æ­¥éª¤2: æ¨¡å‹è®­ç»ƒ (2-2è®­ç»ƒæ¨¡å‹.py)

#### ğŸ§  æ ¸å¿ƒæ¦‚å¿µç†è§£

**ä»€ä¹ˆæ˜¯BERTï¼Ÿ**
- BERTå°±åƒä¸€ä¸ª"è¶…çº§èªæ˜çš„é˜…è¯»ç†è§£AI"
- å®ƒå·²ç»è¯»è¿‡äº’è”ç½‘ä¸Šçš„å¤§é‡æ–‡ç« ï¼Œç†è§£è¯­è¨€çš„å«ä¹‰
- æˆ‘ä»¬åªéœ€è¦æ•™å®ƒåŒºåˆ†åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶

**ä»€ä¹ˆæ˜¯Tokenizerï¼Ÿ**
- æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—ï¼Œå› ä¸ºè®¡ç®—æœºåªè®¤è¯†æ•°å­—
- ä¾‹å¦‚ï¼š"Hello World" â†’ [101, 7592, 2088, 102]

#### ğŸ” é€è¡Œä»£ç è§£é‡Š

```python
# ç¬¬1-2è¡Œï¼šè®¾ç½®ä¸‹è½½é•œåƒï¼ˆè§£å†³ç½‘ç»œé—®é¢˜ï¼‰
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

```python
# ç¬¬4-8è¡Œï¼šå¯¼å…¥éœ€è¦çš„åº“
from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
import pandas as pd

# åº“çš„ä½œç”¨ï¼š
# transformers: æä¾›é¢„è®­ç»ƒçš„AIæ¨¡å‹
# torch: æ·±åº¦å­¦ä¹ æ¡†æ¶
# pandas: æ•°æ®å¤„ç†
```

```python
# ç¬¬10-38è¡Œï¼šè‡ªå®šä¹‰æ•°æ®é›†ç±»
class TextDataset(Dataset):  # ç»§æ‰¿Datasetç±»
    def __init__(self, texts, labels, tokenizer, max_len):
        # æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–æ•°æ®
        self.texts = texts        # æ–‡æœ¬åˆ—è¡¨
        self.labels = labels      # æ ‡ç­¾åˆ—è¡¨  
        self.tokenizer = tokenizer # åˆ†è¯å™¨
        self.max_len = max_len    # æœ€å¤§æ–‡æœ¬é•¿åº¦

    def __len__(self):
        # è¿”å›æ•°æ®é›†å¤§å°
        return len(self.texts)

    def __getitem__(self, idx):
        # è·å–ç¬¬idxä¸ªæ•°æ®é¡¹
        text = self.texts[idx]
        label = self.labels[idx]
        
        # æ–‡æœ¬ç¼–ç ï¼šæŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,    # æ·»åŠ ç‰¹æ®Šæ ‡è®°[CLS], [SEP]
            max_length=self.max_len,    # æœ€å¤§é•¿åº¦128
            return_token_type_ids=False,
            padding='max_length',       # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            truncation=True,           # è¶…é•¿æˆªæ–­
            return_attention_mask=True, # è¿”å›æ³¨æ„åŠ›æ©ç 
            return_tensors='pt',       # è¿”å›PyTorchå¼ é‡
        )
        
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),           # è¾“å…¥ID
            'attention_mask': encoding['attention_mask'].flatten(), # æ³¨æ„åŠ›æ©ç 
            'label': torch.tensor(label, dtype=torch.long)         # æ ‡ç­¾
        }
```

```python
# ç¬¬40-43è¡Œï¼šè¯»å–è®­ç»ƒæ•°æ®
train_data = pd.read_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_train.csv")
texts = train_data['text'].tolist()    # è½¬æ¢ä¸ºPythonåˆ—è¡¨
labels = train_data['label'].tolist()  # è½¬æ¢ä¸ºPythonåˆ—è¡¨
```

```python
# ç¬¬45-47è¡Œï¼šåˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', force_download=True)
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)

# å‚æ•°è§£é‡Šï¼š
# 'albert-base-v2': é¢„è®­ç»ƒæ¨¡å‹åç§°
# num_labels=2: åˆ†ç±»æ•°é‡ï¼ˆåƒåœ¾é‚®ä»¶/æ­£å¸¸é‚®ä»¶ï¼‰
# force_download=True: å¼ºåˆ¶é‡æ–°ä¸‹è½½
```

```python
# ç¬¬49-51è¡Œï¼šæ•°æ®é¢„å¤„ç†
dataset = TextDataset(texts, labels, tokenizer, max_len=128)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# å‚æ•°è§£é‡Šï¼š
# max_len=128: æ–‡æœ¬æœ€å¤§é•¿åº¦
# batch_size=32: æ¯æ‰¹å¤„ç†32ä¸ªæ ·æœ¬
# shuffle=True: éšæœºæ‰“ä¹±æ•°æ®
```

```python
# ç¬¬53-56è¡Œï¼šè®­ç»ƒè®¾ç½®
optimizer = AdamW(model.parameters(), lr=1e-5)  # ğŸš¨ å­¦ä¹ ç‡1e-5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# è§£é‡Šï¼š
# AdamW: ä¼˜åŒ–å™¨ï¼Œç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°
# lr=1e-5: å­¦ä¹ ç‡ï¼Œæ§åˆ¶å­¦ä¹ é€Ÿåº¦
# device: ä½¿ç”¨GPUï¼ˆå¦‚æœæœ‰ï¼‰æˆ–CPU
```

```python
# ç¬¬58-69è¡Œï¼šè®­ç»ƒå¾ªç¯
for epoch in range(1):  # ğŸš¨ é¢˜ç›®è¦æ±‚4è½®ï¼Œè¿™é‡Œå†™1æ˜¯ç­”æ¡ˆçš„ç®€åŒ–ç‰ˆæœ¬
    model.train()       # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    for batch in dataloader:  # éå†æ¯ä¸ªæ‰¹æ¬¡
        # è·å–æ•°æ®å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        # å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹ç»“æœå’ŒæŸå¤±
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        loss.backward()      # è®¡ç®—æ¢¯åº¦
        optimizer.step()     # æ›´æ–°å‚æ•°
        optimizer.zero_grad() # æ¸…é›¶æ¢¯åº¦
```

```python
# ç¬¬71-72è¡Œï¼šä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), "C:/Project/2/2-2model_test.bin")
print('Finished')
```

#### âš ï¸ è€ƒè¯•é‡ç‚¹ä¿®æ”¹ï¼š
```python
# ç¬¬58è¡Œï¼šè®­ç»ƒè½®æ•°æ”¹ä¸º4
for epoch in range(4):  # é¢˜ç›®è¦æ±‚4è½®

# ç¬¬50è¡Œï¼šæ‰¹æ¬¡å¤§å°æ”¹ä¸º10
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # é¢˜ç›®è¦æ±‚æ‰¹æ¬¡å¤§å°10
```

### æ­¥éª¤3: æ¨¡å‹æµ‹è¯• (2-3æµ‹è¯•æ¨¡å‹æ•ˆæœ.py)

#### ğŸ“Š è¯„ä¼°æŒ‡æ ‡è¯¦è§£

```python
# å››ä¸ªå…³é”®æŒ‡æ ‡çš„å«ä¹‰ï¼š
accuracy = accuracy_score(true_labels, predictions)    # å‡†ç¡®ç‡ï¼šé¢„æµ‹å¯¹çš„æ¯”ä¾‹
precision = precision_score(true_labels, predictions)  # ç²¾ç¡®ç‡ï¼šé¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶ä¸­çœŸæ­£æ˜¯åƒåœ¾é‚®ä»¶çš„æ¯”ä¾‹  
recall = recall_score(true_labels, predictions)        # å¬å›ç‡ï¼šæ‰€æœ‰åƒåœ¾é‚®ä»¶ä¸­è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹
f1 = f1_score(true_labels, predictions)               # F1åˆ†æ•°ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

# ä¸¾ä¾‹è¯´æ˜ï¼š
# å‡è®¾æœ‰100å°é‚®ä»¶ï¼Œå…¶ä¸­40å°æ˜¯åƒåœ¾é‚®ä»¶ï¼Œ60å°æ˜¯æ­£å¸¸é‚®ä»¶
# æ¨¡å‹é¢„æµ‹ï¼š35å°åƒåœ¾é‚®ä»¶ï¼Œå…¶ä¸­30å°é¢„æµ‹æ­£ç¡®ï¼Œ5å°é¢„æµ‹é”™è¯¯
#          65å°æ­£å¸¸é‚®ä»¶ï¼Œå…¶ä¸­55å°é¢„æµ‹æ­£ç¡®ï¼Œ10å°é¢„æµ‹é”™è¯¯

# å‡†ç¡®ç‡ = (30+55)/100 = 85%  ï¼ˆæ€»ä½“é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ï¼‰
# ç²¾ç¡®ç‡ = 30/35 = 85.7%      ï¼ˆé¢„æµ‹ä¸ºåƒåœ¾é‚®ä»¶ä¸­çœŸæ­£æ˜¯åƒåœ¾é‚®ä»¶çš„æ¯”ä¾‹ï¼‰
# å¬å›ç‡ = 30/40 = 75%        ï¼ˆæ‰€æœ‰åƒåœ¾é‚®ä»¶ä¸­è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹ï¼‰
# F1åˆ†æ•° = 2*(85.7*75)/(85.7+75) = 80%  ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼‰
```

#### ğŸ” é€è¡Œä»£ç è§£é‡Š

```python
# ç¬¬1-5è¡Œï¼šå¯¼å…¥åº“
import torch
import pandas as pd
from transformers import AlbertTokenizer, AlbertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
```

```python
# ç¬¬7è¡Œï¼šè®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

```python
# ç¬¬9-13è¡Œï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)
model.load_state_dict(torch.load("", map_location=device))  # ğŸš¨ å¡«å…¥æ¨¡å‹è·¯å¾„
model.to(device)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# è§£é‡Šï¼š
# load_state_dict: åŠ è½½æ¨¡å‹å‚æ•°
# map_location=device: æŒ‡å®šåŠ è½½åˆ°å“ªä¸ªè®¾å¤‡
# eval(): è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œå…³é—­dropoutç­‰è®­ç»ƒç‰¹æ€§
```

```python
# ç¬¬15-16è¡Œï¼šåˆå§‹åŒ–åˆ†è¯å™¨
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
```

```python
# ç¬¬18-42è¡Œï¼šæ•°æ®é›†ç±»ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
class TextDataset(Dataset):
    # ... ä»£ç ä¸è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒ
```

```python
# ç¬¬44-47è¡Œï¼šè¯»å–æµ‹è¯•æ•°æ®
test_data = pd.read_csv("")  # ğŸš¨ å¡«å…¥æµ‹è¯•æ•°æ®è·¯å¾„
test_texts = test_data['text'].tolist()
test_labels = test_data['label'].tolist()
```

```python
# ç¬¬49-51è¡Œï¼šåˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_len=128)
test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)

# æ³¨æ„ï¼šshuffle=Falseï¼Œæµ‹è¯•æ—¶ä¸éœ€è¦æ‰“ä¹±æ•°æ®
```

```python
# ç¬¬53-55è¡Œï¼šåˆå§‹åŒ–é¢„æµ‹ç»“æœåˆ—è¡¨
predictions = []
true_labels = []
```

```python
# ç¬¬57-69è¡Œï¼šé¢„æµ‹å¾ªç¯
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
    for batch in test_dataloader:
        # è·å–æ•°æ®
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        # æ¨¡å‹é¢„æµ‹
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits  # è·å–åŸå§‹è¾“å‡º
        preds = torch.argmax(logits, dim=1)  # è·å–é¢„æµ‹ç±»åˆ«
        
        # ä¿å­˜ç»“æœ
        predictions.extend(preds.cpu().numpy())      # è½¬æ¢ä¸ºnumpyæ•°ç»„
        true_labels.extend(labels.cpu().numpy())     # è½¬æ¢ä¸ºnumpyæ•°ç»„

# è§£é‡Šï¼š
# torch.no_grad(): ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
# torch.argmax(logits, dim=1): è·å–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
# .cpu().numpy(): ä»GPUç§»åŠ¨åˆ°CPUå¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„
```

```python
# ç¬¬71-75è¡Œï¼šè®¡ç®—è¯„ä¼°æŒ‡æ ‡
accuracy = accuracy_score(true_labels, predictions)
precision = precision_score(true_labels, predictions)
recall = recall_score(true_labels, predictions)
f1 = f1_score(true_labels, predictions)
```

```python
# ç¬¬77-82è¡Œï¼šä¿å­˜ç»“æœ
with open('', 'w') as f:  # ğŸš¨ å¡«å…¥ç»“æœæ–‡ä»¶è·¯å¾„
    f.write(f"Accuracy: {accuracy}\n")
    f.write(f"Precision: {precision}\n")
    f.write(f"Recall: {recall}\n")
    f.write(f"F1 Score: {f1}\n")

# è§£é‡Šï¼š
# with open() as f: æ‰“å¼€æ–‡ä»¶ï¼Œè‡ªåŠ¨å…³é—­
# f.write(): å†™å…¥æ–‡ä»¶
# f"Accuracy: {accuracy}": f-stringæ ¼å¼åŒ–å­—ç¬¦ä¸²
```

```python
# ç¬¬84-87è¡Œï¼šé”™è¯¯æ ·æœ¬åˆ†æ
error_indices = [i for i, (true, pred) in enumerate(zip(true_labels, predictions)) if true != pred]
error_samples = test_data.iloc[error_indices]
error_samples.to_csv("", index=False)  # ğŸš¨ å¡«å…¥é”™è¯¯åˆ†ææ–‡ä»¶è·¯å¾„

# è§£é‡Šï¼š
# enumerate(zip(true_labels, predictions)): åŒæ—¶éå†çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾
# if true != pred: æ‰¾å‡ºé¢„æµ‹é”™è¯¯çš„æ ·æœ¬
# test_data.iloc[error_indices]: æ ¹æ®ç´¢å¼•è·å–é”™è¯¯æ ·æœ¬
```

#### âš ï¸ è€ƒè¯•å¡«ç©ºç­”æ¡ˆï¼š
```python
# ç¬¬11è¡Œ
model.load_state_dict(torch.load("C:/Project/2/2-2model_test.bin", map_location=device))

# ç¬¬44è¡Œ
test_data = pd.read_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_test.csv")

# ç¬¬78è¡Œ
with open('C:/Project/2/model_test_result.txt', 'w') as f:

# ç¬¬87è¡Œ
error_samples.to_csv("C:/Project/2/error_analysis.txt", index=False)
```

## ğŸš€ è€ƒè¯•åº”è¯•ç­–ç•¥

### 1. è€ƒè¯•æµç¨‹
1. **åˆ›å»ºç›®å½•**: æ‰‹åŠ¨åˆ›å»º `C:/Project/2/` æ–‡ä»¶å¤¹
2. **å¤åˆ¶èµ„æº**: æŠŠèµ„æºåŒ…å¤åˆ¶åˆ°æ¡Œé¢
3. **æŒ‰é¡ºåºå®Œæˆ**: 2-1 â†’ 2-2 â†’ 2-3
4. **æ¯æ­¥æµ‹è¯•**: å®Œæˆä¸€æ­¥å°±è¿è¡Œä¸€æ¬¡ï¼Œç¡®ä¿æ²¡é”™è¯¯

### 2. å…³é”®å¡«ç©ºæ€»ç»“

**2-1åˆ’åˆ†æ•°æ®.py**:
```python
data = pd.read_csv("C:/Users/ç”¨æˆ·å/Desktop/èµ„æºåŒ…/é¢˜ç›®2/é‚®ä»¶æ•°æ®.csv")
os.makedirs("C:/Project/2/", exist_ok=True)
train_data.to_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_train.csv", index=False)
test_data.to_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_test.csv", index=False)
```

**2-2è®­ç»ƒæ¨¡å‹.py** (éœ€è¦æ–°å»º):
- å¤åˆ¶ç­”æ¡ˆæ–‡ä»¶çš„ä»£ç 
- ä¿®æ”¹ `range(1)` ä¸º `range(4)`
- ä¿®æ”¹ `batch_size=32` ä¸º `batch_size=10`

**2-3æµ‹è¯•æ¨¡å‹æ•ˆæœ.py**:
```python
model.load_state_dict(torch.load("C:/Project/2/2-2model_test.bin", map_location=device))
test_data = pd.read_csv("C:/Project/2/æ¸…æ´—åé‚®ä»¶æ•°æ®_test.csv")
with open('C:/Project/2/model_test_result.txt', 'w') as f:
error_samples.to_csv("C:/Project/2/error_analysis.txt", index=False)
```

### 3. å¸¸è§é”™è¯¯é¿å…
1. **è·¯å¾„åˆ†éš”ç¬¦**: Windowsç”¨ `/` æˆ– `\\`ï¼Œæ¨èç”¨ `/`
2. **æ–‡ä»¶åå¤§å°å†™**: ä¸¥æ ¼æŒ‰ç…§é¢˜ç›®è¦æ±‚
3. **ç›®å½•åˆ›å»º**: ç¡®ä¿ `C:/Project/2/` ç›®å½•å­˜åœ¨
4. **å‚æ•°è®¾ç½®**: å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€è®­ç»ƒè½®æ•°è¦ä¸¥æ ¼æŒ‰é¢˜ç›®è¦æ±‚

### 4. è°ƒè¯•æŠ€å·§
```python
# æŸ¥çœ‹æ•°æ®å½¢çŠ¶
print(f"æ•°æ®å½¢çŠ¶: {data.shape}")

# æŸ¥çœ‹å‰å‡ è¡Œ
print(data.head())

# æŸ¥çœ‹æ ‡ç­¾åˆ†å¸ƒ
print(data['label'].value_counts())

# è®­ç»ƒæ—¶æ‰“å°è¿›åº¦
print(f"Epoch {epoch+1}/4 completed")
```

## ğŸ’¡ æœ€åçš„å»ºè®®

1. **ç†è§£æµç¨‹**: æ•°æ®é¢„å¤„ç† â†’ æ¨¡å‹è®­ç»ƒ â†’ æ¨¡å‹è¯„ä¼°
2. **è®°ä½å‚æ•°**: 7:3åˆ’åˆ†ã€å­¦ä¹ ç‡1e-5ã€æ‰¹æ¬¡å¤§å°10ã€è®­ç»ƒ4è½®
3. **æ³¨æ„è·¯å¾„**: æ‰€æœ‰æ–‡ä»¶è·¯å¾„éƒ½è¦æ­£ç¡®
4. **æŒ‰æ­¥éª¤æ¥**: ä¸è¦è·³æ­¥éª¤ï¼Œæ¯æ­¥éƒ½è¦æµ‹è¯•

è®°ä½ï¼šè¿™é“é¢˜çš„æœ¬è´¨å°±æ˜¯**æ–‡æœ¬åˆ†ç±»**ï¼Œç†è§£äº†è¿™ä¸ªæ¦‚å¿µï¼Œä»£ç å°±æ˜¯å·¥å…·è€Œå·²ï¼

---

**ç¥ä½ è€ƒè¯•æˆåŠŸï¼** ğŸ‰