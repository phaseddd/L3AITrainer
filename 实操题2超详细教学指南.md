# 实操题2超详细教学指南 - Python小白版

## 🎯 题目背景
你要做一个**邮件垃圾分类器**，就像QQ邮箱自动把垃圾邮件放到垃圾箱一样。

**数据样例**：
```
text,label
"Win a brand new iPhone 12! Click here to claim your prize now!",1  ← 垃圾邮件
"Hi John, let's catch up for lunch tomorrow at 1 PM.",0              ← 正常邮件
```
- `text`: 邮件内容
- `label`: 0=正常邮件，1=垃圾邮件

## 📚 Python基础语法速成（Java对比）

### 1. 变量和数据类型
```python
# Python - 不需要声明类型
name = "张三"           # 字符串
age = 25               # 整数
scores = [90, 85, 88]  # 列表（相当于Java的ArrayList）
info = {"name": "张三", "age": 25}  # 字典（相当于Java的HashMap）

# Java对比
String name = "张三";
int age = 25;
List<Integer> scores = Arrays.asList(90, 85, 88);
Map<String, Object> info = new HashMap<>();
```

### 2. 导入库（相当于Java的import）
```python
import pandas as pd        # 导入pandas库，起别名pd
from sklearn.model_selection import train_test_split  # 从sklearn库导入特定函数

# Java对比
import java.util.*;
import java.util.List;
```

### 3. 函数定义
```python
# Python
def clean_text(text):      # def = 定义函数
    return text.lower()    # 返回小写文本

# Java对比
public String cleanText(String text) {
    return text.toLowerCase();
}
```

### 4. 循环和条件
```python
# Python - 用缩进表示代码块
for item in my_list:       # 遍历列表
    if item > 10:          # 如果条件
        print(item)        # 打印

# Java对比
for (String item : myList) {
    if (item > 10) {
        System.out.println(item);
    }
}
```

## 🔧 关键Python库详解

### 1. pandas - 数据处理神器
```python
import pandas as pd

# 读取CSV文件（相当于Java读取数据库表）
data = pd.read_csv("文件路径.csv")

# 查看数据
print(data.head())      # 显示前5行
print(data.shape)       # 显示行数和列数 (行, 列)
print(data['text'])     # 获取text列的所有数据

# 应用函数到每一行
data['text'] = data['text'].apply(clean_text)  # 对text列的每个值应用clean_text函数

# 保存文件
data.to_csv("输出文件.csv", index=False)  # index=False表示不保存行号
```

### 2. re - 正则表达式
```python
import re

text = "Hello, World! 123"
# 去除所有非字母数字的字符
clean = re.sub(r'[^\w\s]', '', text)  # 结果: "Hello World 123"
# r'[^\w\s]' 解释：
# r'' = 原始字符串
# [^...] = 不包含...的字符
# \w = 字母数字下划线
# \s = 空格
# 所以这个表达式意思是：删除所有不是字母数字空格的字符
```

### 3. nltk - 自然语言处理
```python
import nltk
from nltk.corpus import stopwords

# 下载停用词表
nltk.download('stopwords')

# 获取英文停用词
stop_words = stopwords.words('english')  # ['the', 'a', 'an', 'and', ...]

# 去除停用词
words = "the quick brown fox".split()  # ['the', 'quick', 'brown', 'fox']
filtered = [word for word in words if word not in stop_words]  # ['quick', 'brown', 'fox']
```

## 📝 三个步骤超详细解析

### 步骤1: 数据预处理 (2-1划分数据.py)

#### 🔍 逐行代码解释

```python
# 第1-8行：导入需要的库
import pandas as pd           # 数据处理库
import re                     # 正则表达式库
import os                     # 操作系统接口库
import nltk                   # 自然语言处理库
nltk.download('stopwords')    # 下载停用词数据

from nltk.corpus import stopwords                    # 导入停用词
from sklearn.model_selection import train_test_split # 导入数据分割函数
```

```python
# 第10行：读取数据
data = pd.read_csv("")  # 🚨 考试时需要填入：桌面/资源包/题目2/邮件数据.csv

# 相当于Java的：
// ResultSet data = statement.executeQuery("SELECT * FROM emails");
```

```python
# 第12-17行：定义文本清洗函数
def clean_text(text):
    # 去除特殊符号：只保留字母、数字、空格
    text = re.sub(r'[^\w\s]', '', text)  
    
    # 转换为小写
    text = text.lower()  
    
    # 去除停用词
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    
    return text

# 详细解释最后一行：
# text.split()                    # 把文本按空格分割成单词列表
# word for word in text.split()   # 遍历每个单词
# if word not in stopwords.words('english')  # 如果单词不在停用词表中
# [...]                          # 列表推导式，生成新列表
# ' '.join([...])                # 把列表重新用空格连接成字符串
```

```python
# 第19行：应用清洗函数
data['text'] = data['text'].apply(clean_text)

# 相当于Java的：
// for (int i = 0; i < data.size(); i++) {
//     data.get(i).setText(cleanText(data.get(i).getText()));
// }
```

```python
# 第21行：数据划分
train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)

# 参数解释：
# data: 要分割的数据
# test_size=0.3: 测试集占30%，训练集占70%
# random_state=42: 随机种子，确保每次分割结果一样
```

```python
# 第23-26行：保存结果
os.makedirs("", exist_ok=True)  # 🚨 填入：C:/Project/2/
train_data.to_csv("", index=False)  # 🚨 填入：C:/Project/2/清洗后邮件数据_train.csv
test_data.to_csv("", index=False)   # 🚨 填入：C:/Project/2/清洗后邮件数据_test.csv

# os.makedirs解释：
# 创建目录，exist_ok=True表示如果目录已存在不报错
```

#### ⚠️ 考试填空答案：
```python
# 第10行
data = pd.read_csv("C:/Users/用户名/Desktop/资源包/题目2/邮件数据.csv")

# 第24行  
os.makedirs("C:/Project/2/", exist_ok=True)

# 第25行
train_data.to_csv("C:/Project/2/清洗后邮件数据_train.csv", index=False)

# 第26行
test_data.to_csv("C:/Project/2/清洗后邮件数据_test.csv", index=False)
```

### 步骤2: 模型训练 (2-2训练模型.py)

#### 🧠 核心概念理解

**什么是BERT？**
- BERT就像一个"超级聪明的阅读理解AI"
- 它已经读过互联网上的大量文章，理解语言的含义
- 我们只需要教它区分垃圾邮件和正常邮件

**什么是Tokenizer？**
- 把文字转换成数字，因为计算机只认识数字
- 例如："Hello World" → [101, 7592, 2088, 102]

#### 🔍 逐行代码解释

```python
# 第1-2行：设置下载镜像（解决网络问题）
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

```python
# 第4-8行：导入需要的库
from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
import pandas as pd

# 库的作用：
# transformers: 提供预训练的AI模型
# torch: 深度学习框架
# pandas: 数据处理
```

```python
# 第10-38行：自定义数据集类
class TextDataset(Dataset):  # 继承Dataset类
    def __init__(self, texts, labels, tokenizer, max_len):
        # 构造函数，初始化数据
        self.texts = texts        # 文本列表
        self.labels = labels      # 标签列表  
        self.tokenizer = tokenizer # 分词器
        self.max_len = max_len    # 最大文本长度

    def __len__(self):
        # 返回数据集大小
        return len(self.texts)

    def __getitem__(self, idx):
        # 获取第idx个数据项
        text = self.texts[idx]
        label = self.labels[idx]
        
        # 文本编码：把文字转换成数字
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,    # 添加特殊标记[CLS], [SEP]
            max_length=self.max_len,    # 最大长度128
            return_token_type_ids=False,
            padding='max_length',       # 填充到最大长度
            truncation=True,           # 超长截断
            return_attention_mask=True, # 返回注意力掩码
            return_tensors='pt',       # 返回PyTorch张量
        )
        
        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),           # 输入ID
            'attention_mask': encoding['attention_mask'].flatten(), # 注意力掩码
            'label': torch.tensor(label, dtype=torch.long)         # 标签
        }
```

```python
# 第40-43行：读取训练数据
train_data = pd.read_csv("C:/Project/2/清洗后邮件数据_train.csv")
texts = train_data['text'].tolist()    # 转换为Python列表
labels = train_data['label'].tolist()  # 转换为Python列表
```

```python
# 第45-47行：初始化模型和分词器
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', force_download=True)
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)

# 参数解释：
# 'albert-base-v2': 预训练模型名称
# num_labels=2: 分类数量（垃圾邮件/正常邮件）
# force_download=True: 强制重新下载
```

```python
# 第49-51行：数据预处理
dataset = TextDataset(texts, labels, tokenizer, max_len=128)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# 参数解释：
# max_len=128: 文本最大长度
# batch_size=32: 每批处理32个样本
# shuffle=True: 随机打乱数据
```

```python
# 第53-56行：训练设置
optimizer = AdamW(model.parameters(), lr=1e-5)  # 🚨 学习率1e-5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 解释：
# AdamW: 优化器，用于更新模型参数
# lr=1e-5: 学习率，控制学习速度
# device: 使用GPU（如果有）或CPU
```

```python
# 第58-69行：训练循环
for epoch in range(1):  # 🚨 题目要求4轮，这里写1是答案的简化版本
    model.train()       # 设置为训练模式
    for batch in dataloader:  # 遍历每个批次
        # 获取数据并移动到设备
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        # 前向传播：计算预测结果和损失
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        
        # 反向传播：计算梯度并更新参数
        loss.backward()      # 计算梯度
        optimizer.step()     # 更新参数
        optimizer.zero_grad() # 清零梯度
```

```python
# 第71-72行：保存模型
torch.save(model.state_dict(), "C:/Project/2/2-2model_test.bin")
print('Finished')
```

#### ⚠️ 考试重点修改：
```python
# 第58行：训练轮数改为4
for epoch in range(4):  # 题目要求4轮

# 第50行：批次大小改为10
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # 题目要求批次大小10
```

### 步骤3: 模型测试 (2-3测试模型效果.py)

#### 📊 评估指标详解

```python
# 四个关键指标的含义：
accuracy = accuracy_score(true_labels, predictions)    # 准确率：预测对的比例
precision = precision_score(true_labels, predictions)  # 精确率：预测为垃圾邮件中真正是垃圾邮件的比例  
recall = recall_score(true_labels, predictions)        # 召回率：所有垃圾邮件中被正确识别的比例
f1 = f1_score(true_labels, predictions)               # F1分数：精确率和召回率的调和平均

# 举例说明：
# 假设有100封邮件，其中40封是垃圾邮件，60封是正常邮件
# 模型预测：35封垃圾邮件，其中30封预测正确，5封预测错误
#          65封正常邮件，其中55封预测正确，10封预测错误

# 准确率 = (30+55)/100 = 85%  （总体预测正确的比例）
# 精确率 = 30/35 = 85.7%      （预测为垃圾邮件中真正是垃圾邮件的比例）
# 召回率 = 30/40 = 75%        （所有垃圾邮件中被正确识别的比例）
# F1分数 = 2*(85.7*75)/(85.7+75) = 80%  （精确率和召回率的调和平均）
```

#### 🔍 逐行代码解释

```python
# 第1-5行：导入库
import torch
import pandas as pd
from transformers import AlbertTokenizer, AlbertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
```

```python
# 第7行：设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

```python
# 第9-13行：加载训练好的模型
model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)
model.load_state_dict(torch.load("", map_location=device))  # 🚨 填入模型路径
model.to(device)
model.eval()  # 设置为评估模式

# 解释：
# load_state_dict: 加载模型参数
# map_location=device: 指定加载到哪个设备
# eval(): 设置为评估模式，关闭dropout等训练特性
```

```python
# 第15-16行：初始化分词器
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
```

```python
# 第18-42行：数据集类（与训练时相同）
class TextDataset(Dataset):
    # ... 代码与训练时完全相同
```

```python
# 第44-47行：读取测试数据
test_data = pd.read_csv("")  # 🚨 填入测试数据路径
test_texts = test_data['text'].tolist()
test_labels = test_data['label'].tolist()
```

```python
# 第49-51行：创建测试数据加载器
test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_len=128)
test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False)

# 注意：shuffle=False，测试时不需要打乱数据
```

```python
# 第53-55行：初始化预测结果列表
predictions = []
true_labels = []
```

```python
# 第57-69行：预测循环
with torch.no_grad():  # 不计算梯度，节省内存
    for batch in test_dataloader:
        # 获取数据
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        # 模型预测
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits  # 获取原始输出
        preds = torch.argmax(logits, dim=1)  # 获取预测类别
        
        # 保存结果
        predictions.extend(preds.cpu().numpy())      # 转换为numpy数组
        true_labels.extend(labels.cpu().numpy())     # 转换为numpy数组

# 解释：
# torch.no_grad(): 禁用梯度计算，节省内存
# torch.argmax(logits, dim=1): 获取概率最大的类别
# .cpu().numpy(): 从GPU移动到CPU并转换为numpy数组
```

```python
# 第71-75行：计算评估指标
accuracy = accuracy_score(true_labels, predictions)
precision = precision_score(true_labels, predictions)
recall = recall_score(true_labels, predictions)
f1 = f1_score(true_labels, predictions)
```

```python
# 第77-82行：保存结果
with open('', 'w') as f:  # 🚨 填入结果文件路径
    f.write(f"Accuracy: {accuracy}\n")
    f.write(f"Precision: {precision}\n")
    f.write(f"Recall: {recall}\n")
    f.write(f"F1 Score: {f1}\n")

# 解释：
# with open() as f: 打开文件，自动关闭
# f.write(): 写入文件
# f"Accuracy: {accuracy}": f-string格式化字符串
```

```python
# 第84-87行：错误样本分析
error_indices = [i for i, (true, pred) in enumerate(zip(true_labels, predictions)) if true != pred]
error_samples = test_data.iloc[error_indices]
error_samples.to_csv("", index=False)  # 🚨 填入错误分析文件路径

# 解释：
# enumerate(zip(true_labels, predictions)): 同时遍历真实标签和预测标签
# if true != pred: 找出预测错误的样本
# test_data.iloc[error_indices]: 根据索引获取错误样本
```

#### ⚠️ 考试填空答案：
```python
# 第11行
model.load_state_dict(torch.load("C:/Project/2/2-2model_test.bin", map_location=device))

# 第44行
test_data = pd.read_csv("C:/Project/2/清洗后邮件数据_test.csv")

# 第78行
with open('C:/Project/2/model_test_result.txt', 'w') as f:

# 第87行
error_samples.to_csv("C:/Project/2/error_analysis.txt", index=False)
```

## 🚀 考试应试策略

### 1. 考试流程
1. **创建目录**: 手动创建 `C:/Project/2/` 文件夹
2. **复制资源**: 把资源包复制到桌面
3. **按顺序完成**: 2-1 → 2-2 → 2-3
4. **每步测试**: 完成一步就运行一次，确保没错误

### 2. 关键填空总结

**2-1划分数据.py**:
```python
data = pd.read_csv("C:/Users/用户名/Desktop/资源包/题目2/邮件数据.csv")
os.makedirs("C:/Project/2/", exist_ok=True)
train_data.to_csv("C:/Project/2/清洗后邮件数据_train.csv", index=False)
test_data.to_csv("C:/Project/2/清洗后邮件数据_test.csv", index=False)
```

**2-2训练模型.py** (需要新建):
- 复制答案文件的代码
- 修改 `range(1)` 为 `range(4)`
- 修改 `batch_size=32` 为 `batch_size=10`

**2-3测试模型效果.py**:
```python
model.load_state_dict(torch.load("C:/Project/2/2-2model_test.bin", map_location=device))
test_data = pd.read_csv("C:/Project/2/清洗后邮件数据_test.csv")
with open('C:/Project/2/model_test_result.txt', 'w') as f:
error_samples.to_csv("C:/Project/2/error_analysis.txt", index=False)
```

### 3. 常见错误避免
1. **路径分隔符**: Windows用 `/` 或 `\\`，推荐用 `/`
2. **文件名大小写**: 严格按照题目要求
3. **目录创建**: 确保 `C:/Project/2/` 目录存在
4. **参数设置**: 学习率、批次大小、训练轮数要严格按题目要求

### 4. 调试技巧
```python
# 查看数据形状
print(f"数据形状: {data.shape}")

# 查看前几行
print(data.head())

# 查看标签分布
print(data['label'].value_counts())

# 训练时打印进度
print(f"Epoch {epoch+1}/4 completed")
```

## 💡 最后的建议

1. **理解流程**: 数据预处理 → 模型训练 → 模型评估
2. **记住参数**: 7:3划分、学习率1e-5、批次大小10、训练4轮
3. **注意路径**: 所有文件路径都要正确
4. **按步骤来**: 不要跳步骤，每步都要测试

记住：这道题的本质就是**文本分类**，理解了这个概念，代码就是工具而已！

---

**祝你考试成功！** 🎉